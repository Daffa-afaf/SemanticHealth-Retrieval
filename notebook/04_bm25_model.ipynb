{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee14b1dd",
   "metadata": {},
   "source": [
    "## BM25 Model - Information Retrieval System\n",
    "Model: BM25Okapi + tokenized preprocessed questions.\n",
    "Tujuan: bangun baseline BM25, fungsi search, evaluasi P@k/Recall/AP/MAP/NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f611c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\acer nitro 5\\appdata\\roaming\\python\\python312\\site-packages (0.2.2)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\acer nitro 5\\appdata\\roaming\\python\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from rank-bm25) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25 Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3418d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded for BM25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries loaded for BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84646e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ..\\data\\processed\\processed_full.csv\n",
      "Loaded shape: (360513, 20)\n",
      "                                              title  \\\n",
      "0                        Khasiat obat zinc sulphate   \n",
      "1                      Perbedaan jenis formula zinc   \n",
      "2  Mengkonsumsi suplemen zinc yang sudah kadaluarsa   \n",
      "\n",
      "                                  processed_question      topic_set  \n",
      "0  khasiat obat zinc sulphate dok mau tanya anak ...  zinc-sulphate  \n",
      "1  beda jenis formula zinc siang dokter dokter sa...  zinc-sulphate  \n",
      "2  konsumsi suplemen zinc kadaluarsa malam dok ba...  zinc-sulphate  \n"
     ]
    }
   ],
   "source": [
    "# Load full processed data (selaras dengan TF-IDF setup)\n",
    "\n",
    "data_path = Path('../data/processed/processed_full.csv')\n",
    "print(f\"Loading: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['processed_question']).reset_index(drop=True)\n",
    "\n",
    "# Ensure text columns are string\n",
    "df['title'] = df['title'].astype(str).fillna('')\n",
    "df['processed_question'] = df['processed_question'].astype(str)\n",
    "df['processed_answer'] = df['processed_answer'].astype(str)\n",
    "\n",
    "print(f\"Loaded shape: {df.shape}\")\n",
    "print(df[['title','processed_question','topic_set']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a5023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menyusun korpus gabungan (title + processed_question) tanpa cleaning tambahan...\n",
      "Total documents: 360,513\n",
      "Sample document (gabungan):\n",
      "Khasiat obat zinc sulphate khasiat obat zinc sulphate dok mau tanya anak kan nak fimosis terus kasih obat zincpro zinc sulphate drops 10mg ml kata buat vitamin lancar bak la terus tak baca kok malah o\n",
      "\n",
      " Dokumen = title + processed_question; tidak ada pembersihan tambahan\n"
     ]
    }
   ],
   "source": [
    "# Susun korpus gabungan: title + processed_question (sudah di-stem dari preprocessing)\n",
    "print(\"Menyusun korpus gabungan (title + processed_question) tanpa cleaning tambahan...\")\n",
    "\n",
    "df['doc_text'] = (df['title'] + ' ' + df['processed_question']).str.strip()\n",
    "documents = df['doc_text'].tolist()\n",
    "\n",
    "print(f\"Total documents: {len(documents):,}\")\n",
    "print(\"Sample document (gabungan):\")\n",
    "print(documents[0][:200])\n",
    "print(\"\\n Dokumen = title + processed_question; tidak ada pembersihan tambahan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2882997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing TF-IDF for relevance construction...\n",
      "TF-IDF (relevance) built in 37.71s | shape=(360513, 30000)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vectorization (untuk konstruksi relevansi yang adil vs TF-IDF notebook)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\nInitializing TF-IDF for relevance construction...\")\n",
    "tfidf_vectorizer_rel = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    min_df=2,\n",
    "    max_df=0.90,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True,\n",
    "    norm='l2',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'(?u)\\b\\w+\\b'\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "tfidf_matrix_rel = tfidf_vectorizer_rel.fit_transform(df['doc_text'].tolist())\n",
    "print(f\"TF-IDF (relevance) built in {time.time()-start_time:.2f}s | shape={tfidf_matrix_rel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de322fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index built\n",
      "Docs: 360,513\n"
     ]
    }
   ],
   "source": [
    "# Tokenize corpus (dokumen sudah di-stem dari preprocessing)\n",
    "tokenized_corpus = [doc.split() for doc in df['doc_text'].tolist()]\n",
    "\n",
    "# Build BM25 dengan default params (baseline approach)\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "print(\"BM25 index built\")\n",
    "print(f\"Docs: {len(tokenized_corpus):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0fbeedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query preprocessing examples:\n",
      "1. anak saya demam tinggi dan batuk -> anak demam tinggi batuk\n",
      "2. bagaimana cara mengatasi asam lambung? -> bagaimana cara atas asam lambung\n",
      "3. Ibu hamil boleh minum obat apa ya dok -> ibu hamil minum obat apa dok\n",
      "4. anak 2 tahun demam 39 derajat -> anak tahun demam derajat\n"
     ]
    }
   ],
   "source": [
    "# Query preprocessing (mirror TF-IDF pipeline)\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stopwords = StopWordRemoverFactory().get_stop_words()\n",
    "def preprocess_query(query):\n",
    "    if pd.isna(query) or query == \"\":\n",
    "        return \"\"\n",
    "    text = str(query).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'\\d+px', '', text)\n",
    "    text = re.sub(r'padding|margin|font|vertical|align', '', text)\n",
    "    text = re.sub(r'\\b\\w*\\d+\\w*\\b', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'\\d{1,2}:\\d{2}', '', text)\n",
    "    text = re.sub(r'wib|wit|wita', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if len(t) > 1 and not any(c.isdigit() for c in t)]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "test_queries = [\n",
    "    \"anak saya demam tinggi dan batuk\",\n",
    "    \"bagaimana cara mengatasi asam lambung?\",\n",
    "    \"Ibu hamil boleh minum obat apa ya dok\",\n",
    "    \"anak 2 tahun demam 39 derajat\"\n",
    "]\n",
    "print(\"\\nQuery preprocessing examples:\")\n",
    "for i,q in enumerate(test_queries,1):\n",
    "    processed = preprocess_query(q)\n",
    "    print(f\"{i}. {q} -> {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5f3ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: anak demam tinggi\n",
      "1. [16.2118] Demam tinggi pada anak\n",
      "2. [15.8561] Penanganan demam tinggi pada anak secara tiba-tiba\n",
      "3. [15.6844] Solusi atasi demam tinggi pada anak\n",
      "\n",
      "Query: sakit kepala dan mual\n",
      "1. [12.1610] sakit kepala disertai mual\n",
      "2. [12.0896] sakit kepala dan mual pada sinusitis\n",
      "3. [12.0202] Apa obat untuk sakit kepala dan mual\n",
      "\n",
      "Query: cara mengatasi asam lambung\n",
      "1. [21.0566] Cara mengatasi asam lambung\n",
      "2. [20.9223] cara mengatasi mual karena asam lambung\n",
      "3. [20.9223] Cara mengatasi gejala asam lambung\n",
      "\n",
      "Query: ibu hamil minum obat\n",
      "1. [15.4001] Bahaya tidak ibu hamil minum obat gatal\n",
      "2. [15.2865] Bolehkah ibu hamil minum obat mabuk?\n",
      "3. [15.2865] Bolehkah ibu hamil minum obat mabuk?\n",
      "\n",
      "Query: batuk berdahak tidak sembuh\n",
      "1. [18.2365] Batuk berdahak putih tidak sembuh-sembuh\n",
      "2. [18.2356] Solusi atasi batuk berdahak pada bayi tak kunjung sembuh\n",
      "3. [17.5963] Penyebab batuk berdahak pada anak tidak kunjung sembuh dan menjadi bat\n"
     ]
    }
   ],
   "source": [
    "def search_bm25(query, top_k=10, preprocessed=False):\n",
    "    processed_query = query if preprocessed else preprocess_query(query)\n",
    "    if not processed_query:\n",
    "        print(\"Empty query after preprocessing\")\n",
    "        return pd.DataFrame()\n",
    "    tokens = processed_query.split()\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    top_scores = scores[top_indices]\n",
    "    results = df.iloc[top_indices].copy()\n",
    "    results['bm25_score'] = top_scores\n",
    "    results['rank'] = range(1, len(results)+1)\n",
    "    cols = ['rank','bm25_score','title','answer','topic_set','answer_count','year']\n",
    "    cols = [c for c in cols if c in results.columns]\n",
    "    return results[cols]\n",
    "test_queries_short = [\n",
    "    \"anak demam tinggi\",\n",
    "    \"sakit kepala dan mual\",\n",
    "    \"cara mengatasi asam lambung\",\n",
    "    \"ibu hamil minum obat\",\n",
    "    \"batuk berdahak tidak sembuh\"\n",
    "]\n",
    "for q in test_queries_short:\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    res = search_bm25(q, top_k=3)\n",
    "    for _,row in res.iterrows():\n",
    "        print(f\"{row['rank']}. [{row['bm25_score']:.4f}] {row['title'][:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7b566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics ready: Precision/Recall/AP/MAP/NDCG\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(relevant_docs, retrieved_docs, k):\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = len(set(retrieved_at_k) & set(relevant_docs))\n",
    "    return relevant_retrieved / k if k > 0 else 0.0\n",
    "def recall_at_k(relevant_docs, retrieved_docs, k):\n",
    "    if len(relevant_docs) == 0:\n",
    "        return 0.0\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_retrieved = len(set(retrieved_at_k) & set(relevant_docs))\n",
    "    return relevant_retrieved / len(relevant_docs)\n",
    "def average_precision(relevant_docs, retrieved_docs, k=None):\n",
    "    if len(relevant_docs) == 0:\n",
    "        return 0.0\n",
    "    if k is None:\n",
    "        k = len(retrieved_docs)\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    for i, doc_id in enumerate(retrieved_at_k, 1):\n",
    "        if doc_id in relevant_docs:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / i)\n",
    "    if len(precisions) == 0:\n",
    "        return 0.0\n",
    "    return sum(precisions) / len(relevant_docs)\n",
    "def ndcg_at_k(relevant_docs, retrieved_docs, k):\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    relevant_set = set(relevant_docs)\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_at_k, 1):\n",
    "        relevance = 1 if doc_id in relevant_set else 0\n",
    "        dcg += relevance / np.log2(i + 1)\n",
    "    m = min(len(relevant_set), k)\n",
    "    idcg = sum(1 / np.log2(i + 1) for i in range(1, m + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "print(\"Metrics ready: Precision/Recall/AP/MAP/NDCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28bedba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Created 30 test queries (BM25-based relevance)\n",
      "Average relevant docs per query: 30.0\n",
      "\n",
      "  Relevansi = top-30 terdekat (BM25 score) dalam topik, excl. diri sendiri\n"
     ]
    }
   ],
   "source": [
    "# Relevansi deterministik (BM25-based): ambil hingga 30 dokumen paling mirip (BM25 score) dalam topik, excl. dirinya\n",
    "k_values = [5, 10, 20]\n",
    "test_topics = df['topic_set'].value_counts().head(10).index.tolist()\n",
    "test_queries_data = []\n",
    "\n",
    "for topic in test_topics:\n",
    "    topic_docs = df[df['topic_set'] == topic]\n",
    "    if len(topic_docs) < 5:\n",
    "        continue\n",
    "    query_samples = topic_docs.sample(n=3, random_state=42)\n",
    "\n",
    "    for idx, query_row in query_samples.iterrows():\n",
    "        topic_indices = topic_docs.index.tolist()\n",
    "        query_tokens = query_row['doc_text'].split()\n",
    "\n",
    "        # Skor BM25 terhadap seluruh korpus, lalu ambil yang satu topik saja\n",
    "        scores = bm25.get_scores(query_tokens)\n",
    "        ranked = [(doc_idx, scores[doc_idx]) for doc_idx in topic_indices if doc_idx != idx]\n",
    "        ranked = sorted(ranked, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Ambil top-30 terdekat sebagai relevan\n",
    "        relevant_indices = [r[0] for r in ranked[:30]]\n",
    "\n",
    "        test_queries_data.append({\n",
    "            'query': query_row['title'],\n",
    "            'query_processed': query_row['processed_question'],\n",
    "            'topic': topic,\n",
    "            'relevant_doc_indices': relevant_indices\n",
    "        })\n",
    "\n",
    "print(f\"\\n Created {len(test_queries_data)} test queries (BM25-based relevance)\")\n",
    "if test_queries_data:\n",
    "    avg_rel = np.mean([len(q['relevant_doc_indices']) for q in test_queries_data])\n",
    "    print(f\"Average relevant docs per query: {avg_rel:.1f}\")\n",
    "    print(\"\\n  Relevansi = top-30 terdekat (BM25 score) dalam topik, excl. diri sendiri\")\n",
    "else:\n",
    "    print(\"No test queries created; cek ukuran topik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 30 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 30/30 [02:52<00:00,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_summary = []\n",
    "print(f\"Evaluating {len(test_queries_data)} queries...\")\n",
    "\n",
    "for query_data in tqdm(test_queries_data, desc=\"Evaluating\"):\n",
    "\n",
    "    query_raw = query_data['query']\n",
    "    query_text = query_data['query_processed']\n",
    "    relevant_indices = set(query_data['relevant_doc_indices'])\n",
    "    search_results = search_bm25(query_text, top_k=max(k_values), preprocessed=True)\n",
    "    if search_results.empty:\n",
    "        continue\n",
    "\n",
    "    retrieved_indices = search_results.index.tolist()\n",
    "\n",
    "    for k in k_values:\n",
    "        retrieved_at_k = retrieved_indices[:k]\n",
    "        hits_k = len(set(retrieved_at_k) & relevant_indices)\n",
    "        success_k = 1 if hits_k > 0 else 0\n",
    "        recall_cap_k = hits_k / min(k, len(relevant_indices)) if len(relevant_indices) > 0 else 0.0\n",
    "\n",
    "        rr_k = 0.0\n",
    "        for rank_pos, doc_id in enumerate(retrieved_at_k, 1):\n",
    "            if doc_id in relevant_indices:\n",
    "                rr_k = 1.0 / rank_pos\n",
    "                break\n",
    "            \n",
    "        precision = precision_at_k(relevant_indices, retrieved_indices, k)\n",
    "        recall = recall_at_k(relevant_indices, retrieved_indices, k)\n",
    "        ap = average_precision(relevant_indices, retrieved_indices, k)\n",
    "        ndcg = ndcg_at_k(relevant_indices, retrieved_indices, k)\n",
    "\n",
    "        results_summary.append({\n",
    "            'query': query_raw[:50],\n",
    "            'topic': query_data['topic'],\n",
    "            'k': k,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'ap': ap,\n",
    "            'ndcg': ndcg,\n",
    "            'num_relevant': len(relevant_indices),\n",
    "            'hits@k': hits_k,\n",
    "            'success@k': success_k,\n",
    "            'recall_cap': recall_cap_k,\n",
    "            'rr@k': rr_k\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "eval_df = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"Evaluation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e8df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Results @ k=5\n",
      "======================================================================\n",
      "Precision@5:   0.3133 ± 0.2609\n",
      "Recall@5:      0.0522 ± 0.0435\n",
      "AP@5:          0.0274 ± 0.0299\n",
      "NDCG@5:        0.2624 ± 0.2192\n",
      "Hits@5:        1.57 (avg count)\n",
      "Success@5:     0.77 (prop query dgn >=1 relevan)\n",
      "RecallCap@5:   0.3133 ± 0.2609\n",
      "MRR@5:         0.3100\n",
      "MAP@5:         0.0274\n",
      "\n",
      "======================================================================\n",
      "Results @ k=10\n",
      "======================================================================\n",
      "Precision@10:   0.3167 ± 0.2451\n",
      "Recall@10:      0.1056 ± 0.0817\n",
      "AP@10:          0.0548 ± 0.0585\n",
      "NDCG@10:        0.2824 ± 0.2169\n",
      "Hits@10:        3.17 (avg count)\n",
      "Success@10:     0.87 (prop query dgn >=1 relevan)\n",
      "RecallCap@10:   0.3167 ± 0.2451\n",
      "MRR@10:         0.3226\n",
      "MAP@10:         0.0548\n",
      "\n",
      "======================================================================\n",
      "Results @ k=20\n",
      "======================================================================\n",
      "Precision@20:   0.3183 ± 0.2002\n",
      "Recall@20:      0.2122 ± 0.1335\n",
      "AP@20:          0.1016 ± 0.0954\n",
      "NDCG@20:        0.2955 ± 0.1925\n",
      "Hits@20:        6.37 (avg count)\n",
      "Success@20:     0.93 (prop query dgn >=1 relevan)\n",
      "RecallCap@20:   0.3183 ± 0.2002\n",
      "MRR@20:         0.3274\n",
      "MAP@20:         0.1016\n",
      "\n",
      "Summary table (dibulatkan 4 desimal):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>ap_mean</th>\n",
       "      <th>ap_std</th>\n",
       "      <th>ndcg_mean</th>\n",
       "      <th>ndcg_std</th>\n",
       "      <th>hit_rate_mean</th>\n",
       "      <th>success_rate_mean</th>\n",
       "      <th>recall_cap_mean</th>\n",
       "      <th>recall_cap_std</th>\n",
       "      <th>mrr_mean</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.2609</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.2624</td>\n",
       "      <td>0.2192</td>\n",
       "      <td>1.5667</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.2609</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.0274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.2451</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.0817</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.2824</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>3.1667</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.2451</td>\n",
       "      <td>0.3226</td>\n",
       "      <td>0.0548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.2122</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.2955</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>6.3667</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.3274</td>\n",
       "      <td>0.1016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k  precision_mean  precision_std  recall_mean  recall_std  ap_mean  \\\n",
       "0   5          0.3133         0.2609       0.0522      0.0435   0.0274   \n",
       "1  10          0.3167         0.2451       0.1056      0.0817   0.0548   \n",
       "2  20          0.3183         0.2002       0.2122      0.1335   0.1016   \n",
       "\n",
       "   ap_std  ndcg_mean  ndcg_std  hit_rate_mean  success_rate_mean  \\\n",
       "0  0.0299     0.2624    0.2192         1.5667             0.7667   \n",
       "1  0.0585     0.2824    0.2169         3.1667             0.8667   \n",
       "2  0.0954     0.2955    0.1925         6.3667             0.9333   \n",
       "\n",
       "   recall_cap_mean  recall_cap_std  mrr_mean     map  \n",
       "0           0.3133          0.2609    0.3100  0.0274  \n",
       "1           0.3167          0.2451    0.3226  0.0548  \n",
       "2           0.3183          0.2002    0.3274  0.1016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Markdown summary (salin ke laporan jika perlu):\n",
      "|   k |   precision_mean |   precision_std |   recall_mean |   recall_std |   ap_mean |   ap_std |   ndcg_mean |   ndcg_std |   hit_rate_mean |   success_rate_mean |   recall_cap_mean |   recall_cap_std |   mrr_mean |    map |\n",
      "|----:|-----------------:|----------------:|--------------:|-------------:|----------:|---------:|------------:|-----------:|----------------:|--------------------:|------------------:|-----------------:|-----------:|-------:|\n",
      "|   5 |           0.3133 |          0.2609 |        0.0522 |       0.0435 |    0.0274 |   0.0299 |      0.2624 |     0.2192 |          1.5667 |              0.7667 |            0.3133 |           0.2609 |     0.31   | 0.0274 |\n",
      "|  10 |           0.3167 |          0.2451 |        0.1056 |       0.0817 |    0.0548 |   0.0585 |      0.2824 |     0.2169 |          3.1667 |              0.8667 |            0.3167 |           0.2451 |     0.3226 | 0.0548 |\n",
      "|  20 |           0.3183 |          0.2002 |        0.2122 |       0.1335 |    0.1016 |   0.0954 |      0.2955 |     0.1925 |          6.3667 |              0.9333 |            0.3183 |           0.2002 |     0.3274 | 0.1016 |\n"
     ]
    }
   ],
   "source": [
    "# Aggregate summary + MAP/MRR\n",
    "if eval_df.empty:\n",
    "    print(\"⚠️ eval_df kosong; pastikan test_queries_data terisi.\")\n",
    "\n",
    "else:\n",
    "    agg_rows = []\n",
    "    map_rows = []\n",
    "\n",
    "    for k in k_values:\n",
    "        \n",
    "        k_results = eval_df[eval_df['k'] == k]\n",
    "        precision_mean = k_results['precision'].mean()\n",
    "        recall_mean = k_results['recall'].mean()\n",
    "        ap_mean = k_results['ap'].mean()\n",
    "        ndcg_mean = k_results['ndcg'].mean()\n",
    "        hit_rate = k_results['hits@k'].mean()\n",
    "        success_rate = k_results['success@k'].mean()\n",
    "        recall_cap_mean = k_results['recall_cap'].mean()\n",
    "        mrr_mean = k_results['rr@k'].mean()\n",
    "\n",
    "        agg_rows.append({\n",
    "            'k': k,\n",
    "            'precision_mean': precision_mean,\n",
    "            'precision_std': k_results['precision'].std(),\n",
    "            'recall_mean': recall_mean,\n",
    "            'recall_std': k_results['recall'].std(),\n",
    "            'ap_mean': ap_mean,\n",
    "            'ap_std': k_results['ap'].std(),\n",
    "            'ndcg_mean': ndcg_mean,\n",
    "            'ndcg_std': k_results['ndcg'].std(),\n",
    "            'hit_rate_mean': hit_rate,\n",
    "            'success_rate_mean': success_rate,\n",
    "            'recall_cap_mean': recall_cap_mean,\n",
    "            'recall_cap_std': k_results['recall_cap'].std(),\n",
    "            'mrr_mean': mrr_mean\n",
    "        })\n",
    "\n",
    "        map_score = ap_mean  # MAP@k = mean AP@k across queries\n",
    "        map_rows.append({'k': k, 'map': map_score})\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Results @ k={k}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Precision@{k}:   {precision_mean:.4f} ± {k_results['precision'].std():.4f}\")\n",
    "        print(f\"Recall@{k}:      {recall_mean:.4f} ± {k_results['recall'].std():.4f}\")\n",
    "        print(f\"AP@{k}:          {ap_mean:.4f} ± {k_results['ap'].std():.4f}\")\n",
    "        print(f\"NDCG@{k}:        {ndcg_mean:.4f} ± {k_results['ndcg'].std():.4f}\")\n",
    "        print(f\"Hits@{k}:        {hit_rate:.2f} (avg count)\")\n",
    "        print(f\"Success@{k}:     {success_rate:.2f} (prop query dgn >=1 relevan)\")\n",
    "        print(f\"RecallCap@{k}:   {recall_cap_mean:.4f} ± {k_results['recall_cap'].std():.4f}\")\n",
    "        print(f\"MRR@{k}:         {mrr_mean:.4f}\")\n",
    "        print(f\"MAP@{k}:         {map_score:.4f}\")\n",
    "\n",
    "    agg_df = pd.DataFrame(agg_rows)\n",
    "    map_df = pd.DataFrame(map_rows)\n",
    "\n",
    "    eval_summary = agg_df.merge(map_df, on='k').sort_values('k')\n",
    "    numeric_cols = ['precision_mean', 'precision_std', 'recall_mean', 'recall_std', 'ap_mean', 'ap_std', 'ndcg_mean', 'ndcg_std', 'hit_rate_mean', 'success_rate_mean', 'recall_cap_mean', 'recall_cap_std', 'mrr_mean', 'map']\n",
    "    eval_summary[numeric_cols] = eval_summary[numeric_cols].apply(lambda s: s.round(4))\n",
    "\n",
    "    print(\"\\nSummary table (dibulatkan 4 desimal):\")\n",
    "    display(eval_summary)\n",
    "\n",
    "    print(\"\\nMarkdown summary (salin ke laporan jika perlu):\")\n",
    "    print(eval_summary.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e409eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BM25 artifacts for deployment (baseline approach)\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "artifacts_dir = Path('../artifacts/bm25')\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(artifacts_dir / 'bm25.pkl', 'wb') as f:\n",
    "    pickle.dump(bm25, f)\n",
    "\n",
    "# Simpan metadata selaras dengan korpus yang diindeks (doc_text)\n",
    "df_meta = df[['title','answer','topic_set','answer_count','year','doc_text','processed_question']].copy()\n",
    "df_meta.to_pickle(artifacts_dir / 'corpus_meta.pkl')\n",
    "\n",
    "with open(artifacts_dir / 'stopwords.json', 'w') as f:\n",
    "    json.dump(stopwords, f)\n",
    "\n",
    "# Save config (default params)\n",
    "config = {'k1': bm25.k1, 'b': bm25.b, 'n_docs': len(df_meta), 'relevance_method': 'BM25-based top-30 within topic'}\n",
    "with open(artifacts_dir / 'bm25_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"✅ BM25 artifacts saved (baseline approach) to {artifacts_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
